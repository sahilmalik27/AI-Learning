# 2-Layer MLP Implementation (Week 2)

## üß† Overview

This directory contains a complete implementation of a 2-layer Multi-Layer Perceptron (MLP) in pure NumPy, trained on MNIST digit classification. It is now powered by a reusable core (`src/nn_core`) for models, optimizers, schedulers, and training loops. This is part of Week 2 of the AI Learning Journey.

## üìÅ Files

| File | Description |
|------|-------------|
| `mlp_mnist_numpy.py` | Main MLP implementation with CLI (uses `nn_core`) |
| `download_mnist_samples.py` | Sample image downloader |
| `mlp_summary.md` | Mathematical documentation |
| `CLI_DOCUMENTATION.md` | Complete CLI reference |
| `CLI_QUICK_REFERENCE.md` | Quick command reference |
| `README.md` | This file |

## üöÄ Quick Start

### 1. Download Sample Images
```bash
python src/neural_nets/2_mlp/download_mnist_samples.py
```

### 2. Train the Model (nn_core-powered)
```bash
python src/neural_nets/2_mlp/mlp_mnist_numpy.py --epochs 10 \
  --opt adamw --lr 3e-3 --weight-decay 1e-2 \
  --lr-sched cosine --warmup-epochs 1 --init he --clip-grad 1.0
```

### 3. Test with Images
## üß© New Use-Cases (2-layer MLP)

All use-cases share the same flags as the main MNIST CLI (`--opt`, `--init`, `--lr-sched`, etc.).

- MNIST:
```bash
python src/neural_nets/2_mlp/mnist.py --epochs 3 --opt adamw --lr 3e-3 \
  --lr-sched cosine --warmup-epochs 1 --weight-decay 1e-2
```

- Fashion-MNIST:
```bash
python src/neural_nets/2_mlp/fashion_mnist.py --epochs 5 --opt adamw --lr 3e-3
```

- CIFAR-10 (flattened baseline; expect lower accuracy):
```bash
python src/neural_nets/2_mlp/cifar10.py --epochs 5 --opt adamw --lr 1e-3
```

- California Housing (regression; baseline MSE path):
```bash
python src/neural_nets/2_mlp/california_housing_regression.py --epochs 10 --opt adamw --lr 1e-2
```

- 20 Newsgroups (TF-IDF + MLP):
```bash
python src/neural_nets/2_mlp/newsgroups_text_classification.py --epochs 5 --opt adamw --lr 3e-3
```
```bash
python src/neural_nets/2_mlp/mlp_mnist_numpy.py --test-all-samples
```

## üéØ Learning Objectives

This implementation demonstrates:

### **Mathematical Concepts**
- ‚úÖ Forward propagation (Linear + ReLU + Softmax)
- ‚úÖ Backpropagation with chain rule
- ‚úÖ Cross-entropy loss function
- ‚úÖ Gradient computation from scratch
- ‚úÖ SGD optimization with weight decay

### **Programming Skills**
- ‚úÖ Pure NumPy implementation
- ‚úÖ Object-oriented design
- ‚úÖ CLI development (backed by `nn_core`)
- ‚úÖ Data visualization
- ‚úÖ Model evaluation

### **Practical Applications**
- ‚úÖ MNIST digit classification
- ‚úÖ Multi-class classification
- ‚úÖ Image-based testing
- ‚úÖ Performance analysis

## üìä Performance

- **Model Size**: 101,770 parameters
- **Training Time**: ~0.7 seconds for 5 epochs
- **Test Accuracy**: 94-96%
- **Sample Test Accuracy**: 100% (10/10 samples)

## üî¨ Architecture

```
Input (784) ‚Üí Hidden (128, ReLU) ‚Üí Output (10, Softmax)
```

- **Input Layer**: 784 units (28√ó28 flattened image)
- **Hidden Layer**: 128 units with ReLU activation
- **Output Layer**: 10 units with Softmax activation
- **Loss Function**: Cross-entropy
- **Optimizer**: SGD with weight decay

## üìö Documentation

### **For Learning**
- `mlp_summary.md` - Mathematical foundations and equations
- `README.md` - This overview

### **For Usage**
- `CLI_DOCUMENTATION.md` - Complete CLI reference (updated for `nn_core`)
- `CLI_QUICK_REFERENCE.md` - Quick command reference (updated)

## üéì Week 2 Progress

### **Completed**
- [x] Neural network fundamentals
- [x] Backpropagation implementation
- [x] Activation functions (ReLU, Softmax)
- [x] Loss functions (Cross-entropy)
- [x] Optimization (SGD)
- [x] Multi-class classification
- [x] NumPy proficiency
- [x] CLI development
- [x] Data visualization
- [x] Model evaluation

### **Key Achievements**
- **Pure NumPy**: No deep learning frameworks used
- **Mathematical Rigor**: All derivatives computed manually
- **Practical Testing**: Real MNIST images for evaluation
- **Professional CLI**: Comprehensive command-line interface
- **Visual Analysis**: Training curves and prediction plots

## üöÄ Next Steps (Week 3)

After mastering this NumPy implementation:
1. **PyTorch Introduction**: Learn tensor operations
2. **Automatic Differentiation**: Use `autograd`
3. **Higher-Level APIs**: `nn.Module`, optimizers
4. **GPU Acceleration**: CUDA support
5. **Modern Architectures**: CNNs, RNNs

## üîß Development

### **Extending the Implementation**
- Add more activation functions (Tanh, Sigmoid)
- Implement different optimizers (Adam, RMSprop)
- Add regularization techniques (Dropout, BatchNorm)
- Create interactive visualizations
- Add model saving/loading
- Add other models to `nn_core/models` (e.g., CNN)

### **Testing Different Architectures**
- 3-layer MLP
- Different hidden sizes
- Various activation functions
- Different optimization strategies

## üìà Results Summary

| Metric | Value |
|--------|-------|
| **Test Accuracy** | 94.65% |
| **Training Time** | 0.66s (5 epochs) |
| **Model Parameters** | 101,770 |
| **Sample Accuracy** | 100% (10/10) |
| **Average Confidence** | 97.2% |

## üéâ Success Criteria Met

- [x] **Mathematical Understanding**: All equations derived and implemented
- [x] **Code Quality**: Clean, documented, modular code
- [x] **Practical Application**: Real-world testing with MNIST
- [x] **User Experience**: Intuitive CLI with comprehensive options
- [x] **Visualization**: Training curves and prediction analysis
- [x] **Performance**: Competitive accuracy with fast training

---

**Ready for Week 3: PyTorch Mastery! üöÄ**
